{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src=\"https://miro.medium.com/v2/resize:fit:1200/1*lbDXL0IuitCRz4mpZ7MmfQ.png\" width=55% > </center>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<center> \n",
    "    <font size=\"6\">Final Lab (Part 2): Image Classification using Convolutional Neural Networks </font>\n",
    "</center>\n",
    "<center> \n",
    "    <font size=\"4\">Computer Vision 1 University of Amsterdam</font> \n",
    "</center>\n",
    "<center> \n",
    "    <font size=\"4\">Due 23:59PM, 24th October, 2025 (Amsterdam time)</font> \n",
    "</center>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "***\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<center>\n",
    "\n",
    "Student1 ID:  \\\n",
    "Student1 Name: \n",
    "\n",
    "Student2 ID: \\\n",
    "Student2 Name: \n",
    "\n",
    "Student3 ID: \\\n",
    "Student3 Name: \n",
    "\n",
    "( Student4 ID: \\\n",
    "Student4 Name: )\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Coding Guidelines**\n",
    "\n",
    "Your code must be handed in this Jupyter notebook, renamed to **StudentID1_StudentID2_StudentID3.ipynb** before the deadline by submitting it to the Canvas Final Lab: Image Classification Assignment. Please also fill out your names and IDs above.\n",
    "\n",
    "For full credit, make sure your notebook follows these guidelines:\n",
    "\n",
    "- Please express your thoughts **concisely**. The number of words does not necessarily correlate with how well you understand the concepts.\n",
    "- Understand the problem as much as you can. When answering a question, provide evidence (qualitative and/or quantitative results, references to papers, figures, etc.) to support your arguments. Not everything might be explicitly asked for, so think about what might strengthen your arguments to make the notebook self-contained and complete.\n",
    "- Tables and figures must be accompanied by a **brief** description. Add a number, a title, and, if applicable, the name and unit of variables in a table, and name and unit of axes and legends in a figure.\n",
    "\n",
    "**Late submissions are not allowed.** Assignments submitted after the strict deadline will not be graded. In case of submission conflicts, TAsâ€™ system clock is taken as reference. We strongly recommend submitting well in advance to avoid last-minute system failure issues.\n",
    "\n",
    "**Environment:** Since this is a project-based assignment, you are free to use any feature descriptor and machine learning tools (e.g., K-means, SVM). You should use Python for your implementation. You are free to use any Python library for this assignment, but make sure to provide a conda environment file!\n",
    "\n",
    "**Plagiarism Note:** Keep in mind that plagiarism (submitted materials which are not your work) is a serious offense and any misconduct will be addressed according to university regulations. This includes using generative tools such as ChatGPT.\n",
    "\n",
    "**Ensure that you save all results/answers to the questions (even if you reuse some code).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Report Preparation**\n",
    "\n",
    "Your tasks include the following:\n",
    "\n",
    "1. **Report Preparation:** For both parts of the final project, students are expected to prepare a report. The report should include all details on implementation approaches, analysis of results for different settings, and visualizations illustrating experiments and performance of your implementation. Grading will be based on the report, so it should be as self-contained as possible. If the report contains faulty results or ambiguities, TAs can refer to your code for clarification. \n",
    "\n",
    "2. **Explanation of Results:** Do not just provide numbers without explanation. Discuss different settings to show your understanding of the material and processes involved.\n",
    "\n",
    "3. **Quantitative Evaluation:** For quantitative evaluation, you are expected to provide the results based on performance (accuracy, learning loss and learning curves). \n",
    "\n",
    "4. **Aim:** Understand the basic Image Classification pipeline using Convolutional Neural Nets (CNN's).\n",
    "\n",
    "5. **Working on Assignments:** Students should work in assigned groups for **two** weeks. Any questions can be discussed on ED.\n",
    "\n",
    "    - **Submission:** Submit your source code and report together in a zip file (`ID1_ID2_ID3_part2.zip`). The report should be a maximum of 10 pages (single-column, including tables and figures, excluding references and appendix). Express thoughts concisely. Tables and figures must be accompanied by a description. Number them and, if applicable, name variables in tables, and label axes in figures.\n",
    "\n",
    "6. **Hyperparameter Search:** In your experiments, remember to perform a hyperparameter search to find the optimal settings for your model(s). Clearly document the search process, the parameters you explored, and how they influenced the performance of your model.\n",
    "\n",
    "8. **Format and Testing:** The report should be in **PDF format**, and the code in **.ipynb format**. Test that all functionality works as expected in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Overview**\n",
    "\n",
    "- [Section 1: Image Classification on CIFAR-100 (0 points)](#section-1)\n",
    "- [Section 2: Visualizing CIFAR-100 Classes and Subclasses (3 points)](#section-2)\n",
    "- [Section 3: TwoLayerNet Architecture (2 points)](#section-3)\n",
    "- [Section 4: ConvNet Architecture (2 points)](#section-4)\n",
    "- [Section 5: Preparation of Training (7 points)](#section-5)\n",
    "- [Section 6: Training the Networks (5 points)](#section-6)\n",
    "- [Section 7: Setting Up the Hyperparameters (14 points)](#section-7)\n",
    "- [Section 8: Visualizing the STL-10 Dataset and Preparing the Data Loader (3 points)](#section-8)\n",
    "- [Section 9: Fine-tuning ConvNet on STL-10 (14 points)](#section-9)\n",
    "- [Section 10: Bonus Challenge (optional)](#section-10)\n",
    "- [Section X: Individual Contribution Report (Mandatory)](#section-x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Section 1: Image Classification on CIFAR-100 (0 points)**\n",
    "\n",
    "The goal of this lab is to implement an image classification system using Convolutional Neural Networks (CNNs) that can identify objects from a set of classes in the [CIFAR-100 dataset](https://www.cs.toronto.edu/~kriz/cifar.html). You will implement and compare two different architectures: a simple two-layer network and a ConvNet based on the LeNet architecture.\n",
    "\n",
    "The CIFAR-100 dataset contains 32x32 pixel RGB images, categorized into 100 different classes. The dataset will be automatically downloaded and loaded using the code provided in this notebook.\n",
    "\n",
    "You will train and test your classification system using the entire CIFAR-100 dataset. Ensure that the test images are excluded from training to maintain a fair evaluation of the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "# Define the transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  \n",
    "])\n",
    "\n",
    "# Load the CIFAR-100 training set\n",
    "train_set = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Load the CIFAR-100 test set\n",
    "test_set = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create data loaders for the entire CIFAR-100 dataset\n",
    "train_data_loader = DataLoader(train_set, shuffle=True)\n",
    "test_data_loader = DataLoader(test_set, shuffle=False)\n",
    "\n",
    "# Define CIFAR-100 superclasses and their subclasses\n",
    "superclasses = {\n",
    "    'aquatic mammals': ['beaver', 'dolphin', 'otter', 'seal', 'whale'],\n",
    "    'fish': ['aquarium_fish', 'flatfish', 'ray', 'shark', 'trout'],\n",
    "    'flowers': ['orchid', 'poppy', 'rose', 'sunflower', 'tulip'],\n",
    "    'food containers': ['bottle', 'bowl', 'can', 'cup', 'plate'],\n",
    "    'fruit and vegetables': ['apple', 'mushroom', 'orange', 'pear', 'sweet_pepper'],\n",
    "    'household electrical devices': ['clock', 'keyboard', 'lamp', 'telephone', 'television'],\n",
    "    'household furniture': ['bed', 'chair', 'couch', 'table', 'wardrobe'],\n",
    "    'insects': ['bee', 'beetle', 'butterfly', 'caterpillar', 'cockroach'],\n",
    "    'large carnivores': ['bear', 'leopard', 'lion', 'tiger', 'wolf'],\n",
    "    'large man-made outdoor things': ['bridge', 'castle', 'house', 'road', 'skyscraper'],\n",
    "    'large natural outdoor scenes': ['cloud', 'forest', 'mountain', 'plain', 'sea'],\n",
    "    'large omnivores and herbivores': ['camel', 'cattle', 'chimpanzee', 'elephant', 'kangaroo'],\n",
    "    'medium-sized mammals': ['fox', 'porcupine', 'possum', 'raccoon', 'skunk'],\n",
    "    'non-insect invertebrates': ['crab', 'lobster', 'snail', 'spider', 'worm'],\n",
    "    'people': ['baby', 'boy', 'girl', 'man', 'woman'],\n",
    "    'reptiles': ['crocodile', 'dinosaur', 'lizard', 'snake', 'turtle'],\n",
    "    'small mammals': ['hamster', 'mouse', 'rabbit', 'shrew', 'squirrel'],\n",
    "    'trees': ['maple_tree', 'oak_tree', 'palm_tree', 'pine_tree', 'willow_tree'],\n",
    "    'vehicles 1': ['bicycle', 'bus', 'motorcycle', 'pickup_truck', 'train'],\n",
    "    'vehicles 2': ['lawn_mower', 'rocket', 'streetcar', 'tank', 'tractor']\n",
    "}\n",
    "\n",
    "# List of all CIFAR-100 classes\n",
    "classes = ('apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 'bicycle', 'bottle', \n",
    "           'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', 'can', 'castle', 'caterpillar', 'cattle',\n",
    "           'chair', 'chimpanzee', 'clock', 'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur',\n",
    "           'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 'house', 'kangaroo', 'keyboard',\n",
    "           'lamp', 'lawn_mower', 'leopard', 'lion', 'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain',\n",
    "           'mouse', 'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear', 'pickup_truck', 'pine_tree',\n",
    "           'plain', 'plate', 'poppy', 'porcupine', 'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', 'sea',\n",
    "           'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider', 'squirrel', 'streetcar', 'sunflower', \n",
    "           'sweet_pepper', 'table', 'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout', 'tulip', \n",
    "           'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm')\n",
    "\n",
    "# Create a mapping of class names to their indices\n",
    "class_to_idx = {cls_name: idx for idx, cls_name in enumerate(classes)}\n",
    "\n",
    "# Create a mapping of superclasses to their corresponding class indices\n",
    "superclass_to_indices = {supcls: [class_to_idx[cls] for cls in subclasses] for supcls, subclasses in superclasses.items()}\n",
    "\n",
    "print(\"Data loaders for CIFAR-100 are ready for use.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-2\"></a>\n",
    "### **Section 2: Visualizing CIFAR-100 Classes and Subclasses (3 points)**\n",
    "\n",
    "In this section, you will implement a function to visualize the CIFAR-100 dataset, including **all** superclasses and their corresponding subclasses. Your implementation should provide a clear and organized overview of the dataset's diversity.\n",
    "\n",
    "You add the figure(s) to appendix of your report and refer to it in the main text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# Fetch the data without the normalization\n",
    "visual_Set = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "found = [] # List of all subclasses for which we have an image\n",
    "images = {} # Lookup for the subclass images\n",
    "idx = 0 # Current index in the visual dataset\n",
    "\n",
    "# Loop through the data until we have an image for all subclasses\n",
    "while(len(found) < 100):\n",
    "    img, target = visual_Set.__getitem__(idx)\n",
    "\n",
    "    # If this is a new subclass, store it\n",
    "    if not (target in found):\n",
    "        found.append(target)\n",
    "        images[target] = img.permute(1, 2, 0)\n",
    "    \n",
    "    idx += 1\n",
    "\n",
    "# Plot the images\n",
    "fig, axs = plt.subplots(nrows=len(superclasses), ncols=int(len(classes)/len(superclasses)), figsize=(5,20), constrained_layout=True)\n",
    "for row, superclass in enumerate(superclasses):\n",
    "\n",
    "    # Plot all the subclasses\n",
    "    for column, subclass in enumerate(superclasses[superclass]):\n",
    "        axs[row, column].axis('off')\n",
    "        axs[row, column].imshow(images[class_to_idx[subclass]])\n",
    "        axs[row, column].set_title(subclass)\n",
    "\n",
    "    # Add superclass name\n",
    "    heigth = 1.0 - (row + 0.5) / len(superclasses)\n",
    "    fig.text(-0.5, heigth, superclass, ha='left', va='center', weight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3\"></a>\n",
    "### **Section 3: TwoLayerNet Architecture (2 points)**\n",
    "\n",
    "In this section, you will implement the architecture of a fully connected neural network called `TwoLayerNet`, consisting of two fully connected layers with a ReLU activation in between. The network accepts an input size of 3x32x32 (CIFAR-100 image), a specified hidden layer size, and the number of output classes. In the `__init__` method, define the first fully connected layer that maps the input size to the hidden size, and the second fully connected layer that maps the hidden size to the number of classes. \n",
    "\n",
    "Ensure to call the parent class constructor using `super(TwoLayerNet, self).__init__()`. In the `forward` method, flatten the input tensor, pass it through the first layer with ReLU activation, and then through the second layer to obtain the final scores.\n",
    "\n",
    "**Note:** You are allowed to modify the provided function definitions as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        '''\n",
    "        Initializes the two-layer neural network model.\n",
    "\n",
    "        Args:\n",
    "            input_size (int): The size of the input features.\n",
    "            hidden_size (int): The size of the hidden layer.\n",
    "            num_classes (int): The number of classes in the dataset.\n",
    "        '''\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "\n",
    "        self.flat = nn.Flatten()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.layer2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        '''\n",
    "        Defines the forward pass of the neural network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor.\n",
    "        '''\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        # Apply the two layers with relu inbetween\n",
    "        x = self.flat(x)\n",
    "        x = self.layer1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.layer2(x)\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-4\"></a>\n",
    "### **Section 4: ConvNet Architecture (2 points)**\n",
    "\n",
    "In this section, you will implement a convolutional neural network inspired by the structure of [LeNet-5](https://ieeexplore.ieee.org/document/726791). The network processes color images using three convolutional layers followed by two fully connected layers. Since you need to feed color images into this network, determine the kernel size of the first convolutional layer. Additionally, calculate the number of trainable parameters in the \"F6\" layer, providing the calculation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        '''\t\n",
    "        Initializes the convolutional neural network model.\n",
    "\n",
    "        Args:\n",
    "            num_classes (int): The number of classes in the dataset.\n",
    "        '''\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        super(ConvNet, self).__init__()\n",
    "\n",
    "        self.layer_C1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=(5, 5))\n",
    "        self.layer_S2 = nn.AvgPool2d(kernel_size=(2, 2))\n",
    "        self.layer_C3 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=(5, 5))\n",
    "        self.layer_S4 = nn.AvgPool2d(kernel_size=(2, 2))\n",
    "        self.layer_C5 = nn.Conv2d(in_channels=16, out_channels=120, kernel_size=(5, 5))\n",
    "        self.flat = nn.Flatten()\n",
    "        self.layer_F6 = nn.Linear(120, 84)\n",
    "        self.gaussian_connection = nn.Linear(84, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Defines the forward pass of the neural network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor.\n",
    "        '''\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        # Apply all the layers and the activation functions (\"sigmoid squashing function\" in the paper)\n",
    "        x = self.layer_C1(x)\n",
    "        x = nn.functional.tanh(x) * 1.7159\n",
    "\n",
    "        x = self.layer_S2(x)\n",
    "\n",
    "        x = self.layer_C3(x)\n",
    "        x = nn.functional.tanh(x) * 1.7159\n",
    "\n",
    "        x = self.layer_S4(x)\n",
    "\n",
    "        x = self.layer_C5(x)\n",
    "        x = nn.functional.tanh(x) * 1.7159\n",
    "\n",
    "        x = self.flat(x)\n",
    "\n",
    "        x = self.layer_F6(x)\n",
    "        x = nn.functional.tanh(x)\n",
    "\n",
    "        x = self.gaussian_connection(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-5\"></a>\n",
    "### **Section 5: Preparation of Training (7 points)**\n",
    "\n",
    "In this section, you will create a custom dataset class to load the CIFAR-100 data, define a transform function for data augmentation, and set up an optimizer for training. While the previous section utilized the built-in CIFAR-100 class from `torchvision`, in practice, you often need to prepare datasets manually. Here, you will implement the `CIFAR100_loader` class to handle the dataset and use `DataLoader` to make it iterable. You will also define a transform function for data augmentation and an optimizer for updating the model's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR100_loader(Dataset):\n",
    "    \n",
    "    def __init__(self, root, train=True, transform=None, download=False):\n",
    "        '''\n",
    "        Initializes the CIFAR-100 dataset loader.\n",
    "\n",
    "        Args:\n",
    "            root (str): The root directory to store the dataset.\n",
    "            train (bool): If True, loads the training data; otherwise, loads the test data.\n",
    "            transform (callable, optional): The data transformations to apply.\n",
    "            download (bool): If True, downloads the dataset if it is not already available.\n",
    "        '''\n",
    "    \n",
    "        # YOUR CODE HERE\n",
    "        self.data_set = torchvision.datasets.CIFAR100(root=root, train=train, download=download, transform=transform)\n",
    "\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Returns the number of samples in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: The number of samples in the dataset.\n",
    "        '''\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        return len(self.data_set)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        Retrieves a sample from the dataset at the specified index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the image and label tensors.\n",
    "        '''\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        return self.data_set[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transforms():\n",
    "    '''\n",
    "    Creates the data transformations for the CIFAR-100 dataset.\n",
    "\n",
    "    Returns:\n",
    "        torchvision.transforms.Compose: The data transformations for the dataset.\n",
    "    '''\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # Values for Normalize come from https://gist.github.com/weiaicunzai/e623931921efefd4c331622c344d8151\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))  \n",
    "    ])\n",
    "\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer(model, learning_rate=0.001):\n",
    "    '''\n",
    "    Creates an optimizer for the model.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The neural network model.\n",
    "        learning_rate (float): The learning rate for the optimizer.\n",
    "\n",
    "    Returns:\n",
    "        torch.optim.Adam: The optimizer for the model.\n",
    "    '''\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-6\"></a>\n",
    "### **Section 6: Training the Networks (5 points)**\n",
    "\n",
    "In this section, you will complete the `train` function and use it to train both the `TwoLayerNet` and `ConvNet` models. You will use the custom `CIFAR100_loader`, transform function, and optimizer function that you implemented. The goal is to compare the performance of the two models on the CIFAR-100 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(net, testloader):\n",
    "    '''\n",
    "    Validates the model on the test dataset.\n",
    "\n",
    "    Args:\n",
    "        net (torch.nn.Module): The neural network model.\n",
    "        testloader (torch.utils.data.DataLoader): The data loader for the test dataset.\n",
    "\n",
    "    Returns:\n",
    "        float: The accuracy of the model on the test dataset.\n",
    "    '''\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    net.eval()\n",
    "\n",
    "    # Determine the device to run the model on\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    # Disable gradient computation\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Iterate over the test dataset\n",
    "        for inputs, labels in testloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy of the network on the test images: {accuracy:.2f} %')\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_per_class(net, testloader, classes):\n",
    "    '''\n",
    "    Validates the model on the test dataset per class.\n",
    "\n",
    "    Args:\n",
    "        net (torch.nn.Module): The neural network model.\n",
    "        testloader (torch.utils.data.DataLoader): The data loader for the test dataset.\n",
    "        classes (tuple): The tuple of class names.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    net.eval()\n",
    "\n",
    "    # Determine the device to run the model on\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    class_correct = [0. for _ in range(len(classes))]\n",
    "    class_total = [0. for _ in range(len(classes))]\n",
    "\n",
    "    # Disable gradient computation\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Iterate over the test dataset\n",
    "        for inputs, labels in testloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions = (predicted == labels).squeeze()\n",
    "\n",
    "            for i in range(len(labels)):\n",
    "                label = labels[i]\n",
    "                class_correct[label] += correct_predictions[i].item()\n",
    "                class_total[label] += 1\n",
    "\n",
    "    for i, class_name in enumerate(classes):\n",
    "        accuracy = 100 * class_correct[i] / class_total[i] if class_total[i] > 0 else 0\n",
    "        print(f'Accuracy of {class_name:5s} : {accuracy:.2f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_loader, criterion, optimizer, epochs=100):\n",
    "    '''\n",
    "    Trains the neural network model.\n",
    "\n",
    "    Args:\n",
    "        net (torch.nn.Module): The neural network model.\n",
    "        train_loader (torch.utils.data.DataLoader): The data loader for the training dataset.\n",
    "        criterion (torch.nn.Module): The loss function.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer for the model.\n",
    "        epochs (int): The number of epochs to train the model.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # Determine the device to run the model on\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Print the training device\n",
    "    print(\"Training on device\", device)\n",
    "\n",
    "    # Set the model to train mode\n",
    "    net.to(device)\n",
    "    net.train()\n",
    "\n",
    "    # Store the average losses of each epoch\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    \n",
    "\n",
    "    # Train the network\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Store the total loss and correct predictions for the epoch to track training progress\n",
    "        total_loss = 0\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "\n",
    "            # Move data to correct device\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Determine logits\n",
    "            outputs = net(inputs)\n",
    "            \n",
    "            # Calculate the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Back propegate\n",
    "            loss.backward()\n",
    "\n",
    "            # Update weigths\n",
    "            optimizer.step()\n",
    "\n",
    "            # Remove the gradients for the next iteration\n",
    "            net.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Update the total loss and the predictions\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Output the average loss and the accuracy of the epoch\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        accuracy = 100 * correct / total\n",
    "        losses.append(avg_loss)\n",
    "        accuracies.append(accuracy)\n",
    "        print(f\"Epoch {epoch} finished with {round(avg_loss, 3)} loss and {round(accuracy, 1)} accuracy.\")\n",
    "\n",
    "    # Plot the learning curve\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(20,8), constrained_layout=True)\n",
    "    axs[0].plot(losses, label='Training loss')\n",
    "    axs[0].set_title('Average loss per epoch versus epoch.')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].set_ylabel('Average loss')\n",
    "    axs[0].legend()\n",
    "\n",
    "    axs[1].plot(accuracies, label='Training accuracy')\n",
    "    axs[1].set_title('Training accuracy per epoch versus epoch.')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].set_ylabel('Accuracy')\n",
    "    axs[1].legend()\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, initialize the datasets and data loaders for both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# Set the seed for reproducability\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Create the models\n",
    "TwoLayerNet_model = TwoLayerNet(input_size=3*32*32, hidden_size=256, num_classes=100)\n",
    "ConvNet_model = ConvNet(num_classes=100)\n",
    "\n",
    "transform = create_transforms()\n",
    "optimizer_twolayermodel = create_optimizer(TwoLayerNet_model)\n",
    "optimizer_convnetmodel = create_optimizer(ConvNet_model)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_dataset = CIFAR100_loader('./data', train=True, transform=transform, download=True)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, train the TwoLayerNet model on the CIFAR-100 dataset using the training data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "train(TwoLayerNet_model, train_dataloader, criterion, optimizer_twolayermodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, train the ConvNet model on the CIFAR-100 dataset using the training data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "train(ConvNet_model, train_dataloader, criterion, optimizer_convnetmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-7\"></a>\n",
    "### **Section 7: Setting Up the Hyperparameters (14 points)**\n",
    "\n",
    "In this section, you will experiment with both the `ConvNet` and `TwoLayerNet` models by setting up and tuning the hyperparameters to achieve the highest possible accuracy. You have the flexibility to modify the training process, including the `train` function, `DataLoader`, `transform` functions, and optimizer as needed.\n",
    "\n",
    "1. Adjust the hyperparameters, including learning rate, batch size, number of epochs, optimizer, weight decay, and transform function to improve the performance of both networks. Modify the training procedure and architecture as necessary. You can also add components like Batch Normalization layers.\n",
    "2. Add two more layers to both `TwoLayerNet` and `ConvNet`. You can decide the size and placement of these layers. Evaluate if these changes result in higher performance and explain your findings.\n",
    "3. Show the final results and describe the modifications made to enhance performance. Discuss the impact of hyperparameter tuning on both `TwoLayerNet` and `ConvNet`.\n",
    "4. Compare the two networks in terms of architecture, performance, and learning rates. Provide a detailed explanation of the differences observed.\n",
    "\n",
    "**Note:** Do not use external pre-trained networks and limit additional convolutional layers to a maximum of three beyond the original architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "import optuna\n",
    "\n",
    "# Modified TwoLayerNet architecture\n",
    "class Modified_TwoLayerNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_classes, dropout=0.5):\n",
    "        '''\n",
    "        Initializes the two-layer neural network model.\n",
    "\n",
    "        Args:\n",
    "            input_size (int): The size of the input features.\n",
    "            hidden_size (int): The size of the hidden layer.\n",
    "            num_classes (int): The number of classes in the dataset.\n",
    "        '''\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        super(Modified_TwoLayerNet, self).__init__()\n",
    "\n",
    "        # The modification is to add two additional linear layers and dropout/batchnorm layers\n",
    "\n",
    "        self.flat = nn.Flatten()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout1 = nn.Dropout1d(p=dropout)\n",
    "\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout2 = nn.Dropout1d(p=dropout)\n",
    "\n",
    "        self.layer3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout3 = nn.Dropout1d(p=dropout)\n",
    "\n",
    "        self.layer4 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        '''\n",
    "        Defines the forward pass of the neural network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor.\n",
    "        '''\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        # Apply the two layers with relu inbetween\n",
    "        x = self.flat(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.layer2(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.layer3(x)\n",
    "        x = self.batchnorm3(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Modified ConvNet architecture\n",
    "class Modified_ConvNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, dropout=0.5):\n",
    "        '''\t\n",
    "        Initializes the convolutional neural network model.\n",
    "\n",
    "        Args:\n",
    "            num_classes (int): The number of classes in the dataset.\n",
    "        '''\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        super(Modified_ConvNet, self).__init__()\n",
    "\n",
    "        self.layer_C1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=(5, 5))\n",
    "        self.batchnorm1 = nn.BatchNorm2d(6)\n",
    "        self.dropout1 = nn.Dropout2d(dropout)\n",
    "\n",
    "        self.layer_S2 = nn.AvgPool2d(kernel_size=(2, 2))\n",
    "\n",
    "        self.layer_C3 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=(5, 5))\n",
    "        self.batchnorm3 = nn.BatchNorm2d(16)\n",
    "        self.dropout3 = nn.Dropout2d(dropout)\n",
    "\n",
    "        self.layer_S4 = nn.AvgPool2d(kernel_size=(2, 2))\n",
    "\n",
    "        self.layer_C5 = nn.Conv2d(in_channels=16, out_channels=120, kernel_size=(5, 5))\n",
    "        self.batchnorm5 = nn.BatchNorm2d(120)\n",
    "        self.dropout5 = nn.Dropout2d(dropout)\n",
    "\n",
    "        self.flat = nn.Flatten()\n",
    "        self.layer_F6 = nn.Linear(120, 84)\n",
    "        self.layer_F7 = nn.Linear(84, 84)\n",
    "        self.layer_F8 = nn.Linear(84, 84)\n",
    "\n",
    "        self.gaussian_connection = nn.Linear(84, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Defines the forward pass of the neural network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor.\n",
    "        '''\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        # Apply all the layers and the activation functions (\"sigmoid squashing function\" in the paper)\n",
    "        x = self.layer_C1(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = nn.functional.tanh(x) * 1.7159\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.layer_S2(x)\n",
    "\n",
    "        x = self.layer_C3(x)\n",
    "        x = self.batchnorm3(x)\n",
    "        x = nn.functional.tanh(x) * 1.7159\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        x = self.layer_S4(x)\n",
    "\n",
    "        x = self.layer_C5(x)\n",
    "        x = self.batchnorm5(x)\n",
    "        x = nn.functional.tanh(x) * 1.7159\n",
    "        x = self.dropout5(x)\n",
    "\n",
    "        x = self.flat(x)\n",
    "\n",
    "        x = self.layer_F6(x)\n",
    "        x = nn.functional.tanh(x) * 1.7159\n",
    "\n",
    "        x = self.layer_F7(x)\n",
    "        x = nn.functional.tanh(x) * 1.7159\n",
    "\n",
    "        x = self.layer_F8(x)\n",
    "        x = nn.functional.tanh(x) * 1.7159\n",
    "        \n",
    "        x = self.gaussian_connection(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Modified training loop. Removes the printing of the loss and accuracy\n",
    "def Modified_train(net, train_loader, criterion, optimizer, epochs=100):\n",
    "    '''\n",
    "    Trains the neural network model.\n",
    "\n",
    "    Args:\n",
    "        net (torch.nn.Module): The neural network model.\n",
    "        train_loader (torch.utils.data.DataLoader): The data loader for the training dataset.\n",
    "        criterion (torch.nn.Module): The loss function.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer for the model.\n",
    "        epochs (int): The number of epochs to train the model.\n",
    "\n",
    "    Returns:\n",
    "        tuple(list, list): The losses and accuracies for the epochs.\n",
    "    '''\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # Determine the device to run the model on\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Set the model to train mode\n",
    "    net.to(device)\n",
    "    net.train()\n",
    "\n",
    "    # Store the average losses of each epoch\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "\n",
    "    # Train the network\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Store the total loss and correct predictions for the epoch to track training progress\n",
    "        total_loss = 0\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "\n",
    "            # Move data to correct device\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Determine logits\n",
    "            outputs = net(inputs)\n",
    "            \n",
    "            # Calculate the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Back propegate\n",
    "            loss.backward()\n",
    "\n",
    "            # Update weigths\n",
    "            optimizer.step()\n",
    "\n",
    "            # Remove the gradients for the next iteration\n",
    "            net.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Update the total loss and the predictions\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Store the average loss and the accuracy of the epoch\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        accuracy = 100 * correct / total\n",
    "        losses.append(avg_loss)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    return losses, accuracies\n",
    "\n",
    "proposal_transforms = {'easy_transform': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))  \n",
    "    ]),\n",
    "    'crop_transform':\n",
    "    transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))  \n",
    "    ])}\n",
    "\n",
    "# Define an objective function for HPO on TwoLayerNet\n",
    "def objective_TwoLayerNet(trial):\n",
    "\n",
    "    # Set seed, device and criterion\n",
    "    torch.manual_seed(0)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Get a value for all the hyper parameters\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n",
    "    weight_decay = trial.suggest_categorical(\"decay\", [0.0, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2])\n",
    "    transform = trial.suggest_categorical(\"transform\", proposal_transforms)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128, 256, 512, 1024])\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0, 1.0)\n",
    "    hidden_size = trial.suggest_categorical(\"hidden_size\", [32, 64, 128, 256])\n",
    "\n",
    "    # Create the model and optimizer\n",
    "    model = Modified_TwoLayerNet(input_size=3*32*32, hidden_size=hidden_size, num_classes=100, dropout=dropout)\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # Create the validation dataset\n",
    "    train_dataset = CIFAR100_loader('./data', train=True, transform=proposal_transforms[transform], download=True)\n",
    "    test_dataset = CIFAR100_loader('./data', train=True, transform=create_transforms(), download=True)\n",
    "    validate_indices = list(range(len(train_dataset)-5000, len(train_dataset)))\n",
    "\n",
    "    # Extract the validation set\n",
    "    train_dataset = torch.utils.data.Subset(train_dataset, validate_indices)\n",
    "    test_dataset = torch.utils.data.Subset(test_dataset, validate_indices)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, shuffle=False)\n",
    "\n",
    "    train_loss, train_acc = Modified_train(model, train_dataloader, criterion, optim, epochs=25)\n",
    "\n",
    "    accuracy = validate(model, test_dataloader)\n",
    "\n",
    "    # Return the accuracy we wish to optimize\n",
    "    return accuracy\n",
    "\n",
    "# Define an objective function for HPO on ConvNet\n",
    "def objective_ConvNet(trial):\n",
    "    \n",
    "    # Set seed, device and criterion\n",
    "    torch.manual_seed(0)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Get a value for all the hyper parameters\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n",
    "    weight_decay = trial.suggest_categorical(\"decay\", [0.0, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2])\n",
    "    transform = trial.suggest_categorical(\"transform\", proposal_transforms)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128, 256, 512, 1024])\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0, 1.0)\n",
    "\n",
    "    # Create the model and optimizer\n",
    "    model = Modified_ConvNet(num_classes=100, dropout=dropout)\n",
    "    model.to(device)\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # Create the validation dataset\n",
    "    train_dataset = CIFAR100_loader('./data', train=True, transform=proposal_transforms[transform], download=True)\n",
    "    test_dataset = CIFAR100_loader('./data', train=True, transform=create_transforms(), download=True)\n",
    "    validate_indices = list(range(len(train_dataset)-5000, len(train_dataset)))\n",
    "\n",
    "    # Extract the validation set\n",
    "    train_dataset = torch.utils.data.Subset(train_dataset, validate_indices)\n",
    "    test_dataset = torch.utils.data.Subset(test_dataset, validate_indices)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, shuffle=False)\n",
    "\n",
    "    train_loss, train_acc = Modified_train(model, train_dataloader, criterion, optim, epochs=25)\n",
    "\n",
    "    accuracy = validate(model, test_dataloader)\n",
    "\n",
    "    # Return the accuracy we wish to optimize\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# Perform the study on TwoLayerNet and save the results\n",
    "study_TwoLayerNet = optuna.create_study(study_name='HPO TwoLayerNet', direction=\"maximize\")\n",
    "study_TwoLayerNet.optimize(objective_TwoLayerNet, n_trials=150)\n",
    "study_TwoLayerNet.trials_dataframe().to_csv('studies/TwoLayerNet.csv')\n",
    "with open('studies/TwoLayerNet_BestParams.txt', 'w') as f:\n",
    "    f.write(study_TwoLayerNet.best_trial.params.__str__())\n",
    "\n",
    "# Perform the study on ConvNet and save the results\n",
    "study_ConvNet = optuna.create_study(study_name='HPO ConvNet', direction=\"maximize\")\n",
    "study_ConvNet.optimize(objective_ConvNet, n_trials=150)\n",
    "study_ConvNet.trials_dataframe().to_csv('studies/ConvNet.csv')\n",
    "with open('studies/ConvNet_BestParams.txt', 'w') as f:\n",
    "    f.write(study_ConvNet.best_trial.params.__str__())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the performance of TwoLayerNet after hyperparameter tuning and compare it with the ConvNet model. Provide a detailed explanation of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the performance of ConvNet after hyperparameter tuning and compare it with the TwoLayerNet model. Provide a detailed explanation of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-8\"></a>\n",
    "### **Section 8: Visualizing the STL-10 Dataset and Preparing the Data Loader (3 points)**\n",
    "\n",
    "In this section, you will work with a subset of the [STL-10](https://cs.stanford.edu/~acoates/stl10/) dataset, containing higher resolution images and different object classes than CIFAR-100. Before fine-tuning your ConvNet on this dataset, first complete the `visualise_stl10` function to display sample images from the following 5 classes:\n",
    "\n",
    "1. **Bird**\n",
    "2. **Deer**\n",
    "3. **Dog**\n",
    "4. **Horse**\n",
    "5. **Monkey**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_stl10(class_mapping):\n",
    "    '''\n",
    "    Visualizes 5 images from each specified class in the STL-10 dataset.\n",
    "\n",
    "    Args:\n",
    "        class_mapping (dict): A dictionary mapping class indices to class names.\n",
    "    '''\n",
    "\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the class mapping for bird, deer, dog, horse, and monkey\n",
    "class_mapping = {1: 'bird', 4: 'deer', 5: 'dog', 6: 'horse', 7: 'monkey'}\n",
    "\n",
    "# Visualize STL-10 classes\n",
    "visualise_stl10(class_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After visualizing the data, implement the `STL10_loader` class to create a custom data loader that initializes the dataset, extracts the target classes, and applies the necessary image transformations. Once these tasks are completed, you will move on to fine-tuning the ConvNet on this dataset in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STL10_loader(Dataset):\n",
    "    def __init__(self, root, train=True, transform=None):\n",
    "        '''\n",
    "        Initializes the STL10 dataset.\n",
    "\n",
    "        Args:\n",
    "            root (str): Root directory of the dataset.\n",
    "            train (bool): If True, use the training set, otherwise use the test set.\n",
    "            transform (callable, optional): A function/transform to apply to the images.\n",
    "        '''\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        self.dataset = torchvision.datasets.STL10()\n",
    "        \n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Returns the number of samples in the dataset.\n",
    "        '''\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        Retrieves a sample from the dataset at the specified index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the transformed image and its target label.\n",
    "        '''\n",
    "\n",
    "        # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-9\"></a>\n",
    "### **Section 9: Fine-tuning ConvNet on STL-10 (14 points)**\n",
    "\n",
    "In this section, you will load the pre-trained parameters of the ConvNet (trained on CIFAR-100) and modify the output layer to adapt it to the new dataset containing 5 classes. You can either first load the pre-trained parameters and then modify the output layer, or change the output layer before loading the matched pre-trained parameters. Once modified, you will train the model and document the settings of hyperparameters, accuracy, and learning curve. Additionally, visualize both the training loss and accuracy to assess the learning process. To gain a deeper understanding of the feature learning process, consider using techniques like [**t-sne**](https://lvdmaaten.github.io/tsne/) for feature space visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-10\"></a>\n",
    "### **Section 10: Bonus Challenge (optional)**\n",
    "\n",
    "Try to achieve the highest possible accuracy on the test dataset (5 classes from STL-10) by adjusting hyperparameters, modifying architectures, or applying techniques like data augmentation. The top-performing teams will earn bonus points that can significantly boost their final lab grade, even allowing it to exceed 10 (up to 11):\n",
    "\n",
    "- **1st place:** +1.0 to the final grade of the final lab\n",
    "- **2nd place:** +0.8 to the final grade of the final lab\n",
    "- **3rd place:** +0.6 to the final grade of the final lab\n",
    "- **4th place:** +0.4 to the final grade of the final lab\n",
    "- **5th place:** +0.2 to the final grade of the final lab\n",
    "\n",
    "**Hint:** You may use techniques like data augmentation, freezing early layers, modifying architecture, or optimizing hyperparameters. Only data from CIFAR-100 and STL-10 can be used, and you cannot add more than 3 additional convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-x\"></a>\n",
    "### **Section X: Individual Contribution Report *(Mandatory)***\n",
    "\n",
    "Because we want each student to contribute fairly to the submitted work, we ask you to fill out the textcells below. Write down your contribution to each of the assignment components in percentages. Naturally, percentages for one particular component should add up to 100% (e.g. 30% - 30% - 40%). No further explanation has to be given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Name | Contribution on Research | Contribution on Programming | Contribution on Writing |\n",
    "| -------- | ------- | ------- | ------- |\n",
    "|  | - % | - % | - % |\n",
    "|  | - % | - % | - % |\n",
    "|  | - % | - % | - % |\n",
    "|  | - % | - % | - % |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - End of Notebook -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torchinfo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
